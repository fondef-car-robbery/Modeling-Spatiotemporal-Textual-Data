{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RepresentRNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/juglar-diaz/STTD/blob/master/RepresentRNN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "c02j_-p6TZ2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Intro"
      ]
    },
    {
      "metadata": {
        "id": "R3QjBnH6lktK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJWrJduhlnI-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGPfqOgRnsBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "9f60a73d-2081-413f-9983-b310089c17c6"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip3 install torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 32kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58882000 @  0x7f23ee8af1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.7MB/s \n",
            "\u001b[?25hCollecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.5MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torch, pillow, torchvision, torchtext\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.2.0 torch-0.4.1 torchtext-0.2.3 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lboU3CzKkHsc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import itertools\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "sep = os.sep\n",
        "import os.path\n",
        "\n",
        "data = \"\"\n",
        "import pandas as pd\n",
        "import bisect\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "id7ExeAEnvbv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    inGPU = True\n",
        "    _type = torch.cuda.FloatTensor\n",
        "    _typelong = torch.cuda.LongTensor\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    inGPU = False\n",
        "    _type = torch.FloatTensor\n",
        "    _typelong = torch.LongTensor\n",
        "    device = torch.device(\"cpu\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s6m5UOXClc5v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data"
      ]
    },
    {
      "metadata": {
        "id": "Nrks3-MplPnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exchangedrive = drive.CreateFile({'id':'1cMCzlvTMlUPgaYaUIdlMVtTpp8r10GnX'})\n",
        "exchangedrive.GetContentFile('robosclean.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fqinedc4lvh2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exchangedrive = drive.CreateFile({'id':'1A5Wa6LiaGs8XeW2S2qjGbcoSSlsMGW97'})\n",
        "exchangedrive.GetContentFile('tweetsLAtrain.csv')\n",
        "exchangedrive = drive.CreateFile({'id':'1CrUCS7oWzdvYoFwtWgikiGkDAu6w3dOF'})\n",
        "exchangedrive.GetContentFile('tweetsLAtest.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYQvo_-Alvl_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exchangedrive = drive.CreateFile({'id':'1NuSVM7-h0CCRtzi0woM4h5tVd6T7JO2C'})\n",
        "exchangedrive.GetContentFile('tweetsNYtrain.csv')\n",
        "exchangedrive = drive.CreateFile({'id':'1UYZY0sh1-Q8MIofMAHDGaNKv8cfDA0ui'})\n",
        "exchangedrive.GetContentFile('tweetsNYtest.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w5iLxHb8lvqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exchangedrive = drive.CreateFile({'id':'1Jv19eJTZwsZWEA_rUudvKwcn1TS-DFBa'})\n",
        "exchangedrive.GetContentFile('tweets2016_2half.csv')\n",
        "exchangedrive = drive.CreateFile({'id':'1E8WlhOXb3tfQbLUpizx7QXzeydv57eXN'})\n",
        "exchangedrive.GetContentFile('toy_2017_Jan.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "42Nlwku5krUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def buildIndexData(list_elements, start_index = 0):\n",
        "\n",
        "    idx2data = {index + start_index: discretization for index, discretization in enumerate(set(list_elements))}\n",
        "\n",
        "    data2idx = {discretization: index for index, discretization in idx2data.items()}\n",
        "\n",
        "    indexes = [data2idx[element] for element in list_elements]\n",
        "\n",
        "    return indexes, data2idx, idx2data\n",
        "\n",
        "\n",
        "class Discretize:\n",
        "    def fit_transform(self):\n",
        "        pass\n",
        "\n",
        "    def transform(self):\n",
        "        pass\n",
        "\n",
        "    def updateIndexes(self, indexes, star_index):\n",
        "        map_indexes = {value:counter+star_index for counter, value in enumerate(set(indexes))}\n",
        "\n",
        "\n",
        "        new_indexes = [map_indexes[index] for index in indexes]\n",
        "\n",
        "        idx2data = {map_indexes[index]:self.idx_data[index] for index in set(indexes)}\n",
        "        data2idx = {val:key for (key, val) in idx2data.items()}\n",
        "\n",
        "        self.idx_data = idx2data\n",
        "        self.data_idx = data2idx\n",
        "\n",
        "        return new_indexes\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpcjzVxBkKbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Round(Discretize):\n",
        "    def __init__(self, div= 10):\n",
        "        self.div= div\n",
        "\n",
        "    def fit_transform(self, latitudes, longitudes, start_index = 0, vocab_size = -1, vocab_min_count=0):\n",
        "        lat = (latitudes * self.div).astype(int)\n",
        "        lon = (longitudes * self.div).astype(int)\n",
        "\n",
        "        discretizations = list(zip(lat, lon))\n",
        "\n",
        "        counter = Counter(discretizations)\n",
        "        if (vocab_size > 0):\n",
        "            pairs = counter.most_common(vocab_size)\n",
        "        else:\n",
        "            pairs = list(counter.items())\n",
        "\n",
        "        self.vocab = [keyword for keyword, count in pairs if count >= vocab_min_count]\n",
        "\n",
        "        self.indexes, self.data_idx, self.idx_data = buildIndexData(self.vocab, start_index)\n",
        "\n",
        "\n",
        "\n",
        "        return [self.data_idx.get(data, None) for data in discretizations], self.data_idx, self.idx_data\n",
        "\n",
        "\n",
        "\n",
        "    def transform(self, latitudes, longitudes):\n",
        "        lat = (latitudes * self.div).astype(int)\n",
        "        lon = (longitudes * self.div).astype(int)\n",
        "\n",
        "        discretizations = list(zip(lat, lon))\n",
        "        #indexes = [self.data_idx.get(data, None) for data in discretizations]\n",
        "        #return indexes\n",
        "        return discretizations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETcm1_8ikKel",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HourofDay(Discretize):\n",
        "    def fit_transform(self, created_at, start_index = 0):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "        discretizations = list(indi.hour)\n",
        "        self.indexes, self.data_idx, self.idx_data = buildIndexData(discretizations, start_index)\n",
        "        return self.indexes, self.data_idx, self.idx_data\n",
        "\n",
        "\n",
        "    def transform(self, created_at):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "        discretizations = list(indi.hour)\n",
        "        #indexes = [self.data_idx.get(data,None) for data in discretizations]\n",
        "        #return indexes\n",
        "        return discretizations\n",
        "\n",
        "class DayofWeak(Discretize):\n",
        "    def fit_transform(self, created_at, start_index = 0):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "\n",
        "        discretizations = list(indi.weekday)\n",
        "        self.indexes, self.data_idx, self.idx_data = buildIndexData(discretizations, start_index)\n",
        "        return self.indexes, self.data_idx, self.idx_data\n",
        "\n",
        "    def transform(self, created_at):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "        discretizations = list(indi.weekday)\n",
        "        indexes = [self.data_idx.get(data,None) for data in discretizations]\n",
        "        return indexes\n",
        "\n",
        "\n",
        "class HourofDay_DayofWeak(Discretize):\n",
        "    def fit_transform(self, created_at, start_index = 0):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "\n",
        "        discretizations = list(zip(indi.weekday, indi.hour))\n",
        "        self.indexes, self.data_idx, self.idx_data = buildIndexData(discretizations, start_index)\n",
        "        return self.indexes, self.data_idx, self.idx_data\n",
        "\n",
        "    def transform(self, created_at):\n",
        "        indi = pd.DatetimeIndex(created_at)\n",
        "\n",
        "        discretizations = list(zip(indi.weekday, indi.hour))\n",
        "        indexes = [self.data_idx.get(data, None) for data in discretizations]\n",
        "        return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83kiVYqij4qA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class Indexer():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_transform(self,\n",
        "            filename,\n",
        "            time_discretizer = HourofDay,\n",
        "            coor_discretizer = Round,\n",
        "            #represent_text = RepresentText,\n",
        "            dates_vocab_size = 0, dates_vocab_mincount = 0,\n",
        "            places_vocab_size = 0, places_vocab_mincount = 0,\n",
        "            words_vocab_size = 0, words_vocab_mincount = 0): #file_csv has columns created_at, latitude, longitude, text\n",
        "\n",
        "        #self.datapath = \"Data\" + sep\n",
        "        if (filename.split('.')[1] == 'csv'):\n",
        "            df = pd.read_csv(filename)\n",
        "        elif (filename.split('.')[1] == 'p'):\n",
        "            df = pd.read_pickle(filename)\n",
        "\n",
        "        self.time_discretizer = time_discretizer()\n",
        "        self.coor_discretizer = coor_discretizer(100)\n",
        "        #self.represent_text = represent_text()\n",
        "\n",
        "        #date_out, self.date_idx, self.idx_date = self.time_discretizer.fit_transform(df['created_at'], start_index = 0)\n",
        "        dates = self.time_discretizer.transform(df['created_at'])\n",
        "\n",
        "        #self.coor_out, self.coor_idx, self.idx_coor = self.coor_discretizer.fit_transform(df['latitude'], df['longitude'],start_index=max(self.date_out) + 1)\n",
        "\n",
        "        places = self.coor_discretizer.transform(df['latitude'], df['longitude'])\n",
        "\n",
        "        \n",
        "        texts = df['texts'].astype(str)\n",
        "        words = [word for list_words in texts for word in list_words.split()]\n",
        "        #print(len(texts))\n",
        "\n",
        "\n",
        "        counter_dates = Counter(dates)\n",
        "        if (dates_vocab_size > 0):\n",
        "            pairs = counter_dates.most_common(dates_vocab_size)\n",
        "        else:\n",
        "            pairs = list(counter_dates.items())\n",
        "        self.vocab_dates = set([date for date, count in pairs if count >= dates_vocab_mincount])\n",
        "\n",
        "\n",
        "        counter_places = Counter(places)\n",
        "        if (places_vocab_size > 0):\n",
        "            pairs = counter_places.most_common(places_vocab_size)\n",
        "        else:\n",
        "            pairs = list(counter_places.items())\n",
        "        self.vocab_places = set([place for place, count in pairs if count >= places_vocab_mincount])\n",
        "\n",
        "\n",
        "        counter_words = Counter(words)\n",
        "        if (words_vocab_size > 0):\n",
        "            pairs = counter_words.most_common(words_vocab_size)\n",
        "        else:\n",
        "            pairs = list(counter_words.items())\n",
        "\n",
        "        self.vocab_words = set([keyword for keyword, count in pairs if count >= words_vocab_mincount])\n",
        "\n",
        "        filtered_dates = set([i for i in range(len(dates)) if dates[i] in self.vocab_dates ])\n",
        "        filtered_places = set([i for i in range(len(places)) if places[i] in self.vocab_places])\n",
        "        filtered_words = set([i for i in range(len(texts)) if any([word in self.vocab_words for word in texts[i].split()]) ])\n",
        "\n",
        "        #print(len(filtered_dates))\n",
        "        #print(len(filtered_places))\n",
        "        #print(len(filtered_words))\n",
        "\n",
        "        filtered = list(filtered_dates.intersection(filtered_places).intersection(filtered_words))\n",
        "        #print(len(filtered))\n",
        "\n",
        "        dates = [dates[i] for i in filtered]\n",
        "        places = [places[i] for i in filtered]\n",
        "        texts = [texts[i] for i in filtered]\n",
        "\n",
        "\n",
        "        idxsdates, self.date2idx, self.idx2date = buildIndexData(dates, start_index=0)\n",
        "        idxsplaces, self.place2idx, self.idx2place = buildIndexData(places, start_index=max(idxsdates) + 1)\n",
        "\n",
        "        idxs, self.word2idx, self.idx2word = buildIndexData(self.vocab_words, start_index = max(idxsplaces) + 1)\n",
        "\n",
        "        idxstexts = []\n",
        "\n",
        "        for text in texts:\n",
        "            indexed_text = [self.word2idx[word] for word in text.split() if word in self.vocab_words]\n",
        "            idxstexts.append(indexed_text)\n",
        "\n",
        "        self.idx2item = {}\n",
        "        self.idx2item.update(self.idx2word)\n",
        "        self.idx2item.update(self.idx2place)\n",
        "        self.idx2item.update(self.idx2date)\n",
        "\n",
        "        self.item2idx = {}\n",
        "        self.item2idx.update(self.word2idx)\n",
        "        self.item2idx.update(self.place2idx)\n",
        "        self.item2idx.update(self.date2idx)\n",
        "\n",
        "        return list(zip(idxsdates,\n",
        "                             idxsplaces,\n",
        "                             idxstexts))\n",
        "\n",
        "\n",
        "    def transform(self, filename):\n",
        "        if (filename.split('.')[1] == 'csv'):\n",
        "            df = pd.read_csv( filename)\n",
        "        elif (filename.split('.')[1] == 'p'):\n",
        "            df = pd.read_pickle( filename)\n",
        "\n",
        "        dates = self.time_discretizer.transform(df['created_at'])\n",
        "        idxsdates = [self.date2idx.get(date, None) for date in dates]\n",
        "\n",
        "        places = self.coor_discretizer.transform(df['latitude'], df['longitude'])\n",
        "        idxsplaces = [self.place2idx.get(place, None) for place in places]\n",
        "\n",
        "        idxstexts = []\n",
        "        for text in df['texts'].astype(str):\n",
        "            indexed_text = [self.word2idx[word] for word in text.split() if word in self.vocab_words]\n",
        "            idxstexts.append(indexed_text)\n",
        "\n",
        "\n",
        "        full_list = list(zip(idxsdates,\n",
        "                             idxsplaces,\n",
        "                             idxstexts))\n",
        "        #print(len(full_list))\n",
        "        clean_list = [(x[0], x[1], x[2]) for x in full_list if ((x[0] != None) and (x[1] != None) and (x[2] != [])) ]\n",
        "        return clean_list\n",
        "\n",
        "\n",
        "    def Item2index(self, item):\n",
        "        return self.item2idx.get(item, -1)\n",
        "\n",
        "    def Index2item(self, index):\n",
        "        return self.idx2item.get(index, None)\n",
        "\n",
        "    def indexes(self):\n",
        "        return (self.coor_out[0], self.coor_out[-1], self.texts_out[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Nr0OFbBnUOj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2b9e1895-e36b-4289-caca-1e6e423c8d26"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "indexerST = Indexer()\n",
        "trainST = indexerST.fit_transform(data+'tweets2016_2half.csv', dates_vocab_mincount=0, words_vocab_mincount=100, places_vocab_mincount=10)\n",
        "testST = indexerST.transform(data+'toy_2017_Jan.csv')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.2 s, sys: 242 ms, total: 11.4 s\n",
            "Wall time: 11.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wO76vzQhnUSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8036f61f-bf03-4c5d-fd89-765289c39291"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "indexerLA = Indexer()\n",
        "trainLA = indexerLA.fit_transform(data+'tweetsLAtrain.csv', dates_vocab_mincount=0, words_vocab_mincount=100, places_vocab_mincount=10)\n",
        "testLA = indexerLA.transform(data+'tweetsLAtest.csv')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 41.6 s, sys: 839 ms, total: 42.5 s\n",
            "Wall time: 42.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ehoAepDonUWB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6e8f93f6-4a95-4381-d918-2fcea4bbdaf4"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "indexerNY = Indexer()\n",
        "trainNY = indexerNY.fit_transform(data+'tweetsNYtrain.csv', dates_vocab_mincount=0, words_vocab_mincount=100, places_vocab_mincount=10)\n",
        "testNY = indexerNY.transform(data+'tweetsNYtest.csv')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.9 s, sys: 101 ms, total: 12 s\n",
            "Wall time: 12 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bbiGFbPwveHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e913bc66-1155-4bf6-8813-3fe9deb5d90d"
      },
      "cell_type": "code",
      "source": [
        "t = torch.zeros([150,2,100])\n",
        "print(t.shape)\n",
        "print(torch.sum(t, dim=1).shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([150, 2, 100])\n",
            "torch.Size([150, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L1VwYzyQT_Ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ce8da82e-3158-41e7-9fef-02863885e22d"
      },
      "cell_type": "code",
      "source": [
        "t = torch.ones([3,2])\n",
        "v = torch.tensor([[2.0],[1.5],[3.0]])\n",
        "print(v.shape)\n",
        "print(t.shape)\n",
        "z = v * t\n",
        "z.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 1])\n",
            "torch.Size([3, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "fKDUM7jvCQJb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Models"
      ]
    },
    {
      "metadata": {
        "id": "7S5_mCHKVqul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "      super(Embed, self).__init__()\n",
        "      self.embed_dim = embedding_dim\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "  def forward(self, input):\n",
        "      return self.embedding(input)\n",
        "        \n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, embed):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.embedding = embed\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        #print(input)\n",
        "        #print(embedded)\n",
        "        \n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    \n",
        "      \n",
        "      \n",
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, embed, range_times, range_coors, range_words):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = embed\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        \n",
        "        self.linearTimes = nn.Linear(hidden_size, max(range_times)-min(range_times)+1, bias=False)\n",
        "        self.linearCoors = nn.Linear(hidden_size, max(range_coors)-min(range_coors)+1, bias=False)\n",
        "        self.linearWords = nn.Linear(hidden_size, max(range_words)-min(range_words)+1, bias=False)\n",
        "        \n",
        "    def forward(self, inputs, hidden, target):\n",
        "      \n",
        "        #embeds = self.embedding(inputs).view(1, 1, -1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #embeds = embeds.view((self.batch_size,-1))\n",
        "        \n",
        "        \n",
        "        if target == 0:\n",
        "          out = self.linearTimes(hidden)\n",
        "          #print(hidden)\n",
        "          #print(out)\n",
        "\n",
        "          log_probs = self.softmax(out.view(1,-1))\n",
        "          #print(log_probs)\n",
        "          return log_probs.view(1,-1)#.view(log_probs.numel())  \n",
        "\n",
        "        elif target == 1:\n",
        "          out = self.linearCoors(hidden)\n",
        "          log_probs = self.softmax(out.view(1,-1))\n",
        "\n",
        "          return log_probs.view(1,-1)#.view(log_probs.numel())            \n",
        "                        \n",
        "\n",
        "        else:#target in range_words\n",
        "          \n",
        "          embeds = self.embedding(inputs).view(1, 1, -1)\n",
        "          #print(inputs.shape)\n",
        "          #print(hidden.shape)\n",
        "          \n",
        "          output, hidden = self.gru(embeds, hidden)\n",
        "          #print(output.shape)\n",
        "          \n",
        "          output = self.softmax(self.linearWords(output[0]))\n",
        "          return output, hidden\n",
        "        \n",
        "      \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AJH1LJyznUgU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleEmbedding():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def transform_as_nparray(self, tuples_train):\n",
        "        output = []\n",
        "        self.max_length = 0\n",
        "        for (aux_date_idx, aux_coor_idx, text) in tuples_train:\n",
        "            if(len(text) > self.max_length):\n",
        "              self.max_length = len(text)\n",
        "            \n",
        "            for word_numb in text:\n",
        "                _tuple = (aux_date_idx, aux_coor_idx, word_numb)\n",
        "                output.append(_tuple)\n",
        "        print(self.max_length)\n",
        "        return np.array(output)\n",
        "    \n",
        "    def transform_as_seq(self, tuples_train):\n",
        "        output = {0:[], 1:[], 2:[]}\n",
        "        for _tuple in tuples_train:\n",
        "                _tuple2 = _tuple[2]\n",
        "                for i in range(self.max_length-len(_tuple2)):\n",
        "                    _tuple2.append(self.padding)\n",
        "                \n",
        "                tuple = ([_tuple[0]],[_tuple[1]],_tuple[2])\n",
        "                    \n",
        "                for i in range(3):\n",
        "                    target = [t-self.min_ranges[i] for t in tuple[i]]\n",
        "                     \n",
        "                    data = [k for j in range(3) if j != i for k in tuple[j]]\n",
        "                    output[i].append((data, target))\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    \n",
        "    \n",
        "    def get_input_layer(self, data, vocabulary_size):\n",
        "        x = torch.zeros(vocabulary_size).float()\n",
        "        for i in data:\n",
        "            x[i] = 1.0\n",
        "        return x\n",
        "    \n",
        "    def next_batch(self):\n",
        "        numexamples = self.xytrain.shape[0]\n",
        "        positions = random.sample(range(numexamples),self.batch_size)\n",
        "        target = random.sample(range(3),1)\n",
        "        inputs = [j for j in range(3) if j != target[0]]\n",
        "        \n",
        "        x = self.xytrain[positions][:,inputs]\n",
        "        y = self.xytrain[positions][:,target[0]]\n",
        "        y = y-self.min_ranges[target[0]]\n",
        "        return x, y, target[0]\n",
        "    \n",
        "    \n",
        "    def get_ranks(self, test, predictor, predict_type = 'w'):\n",
        "        self.predict_type = predict_type\n",
        "        noiseList = np.random.choice(len(test), self.fake_num*len(test)).tolist()\n",
        "        count = 5\n",
        "        for example in test:\n",
        "\n",
        "\n",
        "\n",
        "            scores = []\n",
        "            score = predictor.predict(example[0], example[1], example[2])\n",
        "            scores.append(score)\n",
        "\n",
        "\n",
        "            for i in range(self.fake_num):\n",
        "                noise = test[noiseList.pop()]\n",
        "                if self.predict_type == 't':\n",
        "                    noise_score = predictor.predict(noise[0], example[1], example[2])\n",
        "                elif self.predict_type=='l':\n",
        "                    noise_score = predictor.predict(example[0], noise[1], example[2])\n",
        "                elif self.predict_type=='w':\n",
        "                    noise_score = predictor.predict(example[0], example[1], noise[2])\n",
        "                scores.append(noise_score)\n",
        "            scores.sort()\n",
        "    \n",
        "    \n",
        "    def fit(self, tuples_train, embedding_dims, num_epochs = 1, hidden_size=100):\n",
        "        xytrain = self.transform_as_nparray(tuples_train)\n",
        "        vocabulary_size = int(max(list(xytrain[:, -1]))) + 1\n",
        "        \n",
        "        self.padding = vocabulary_size\n",
        "        vocabulary_size += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "        range_times = range(int(max(list(xytrain[:, -3]))) + 1)\n",
        "        range_coors = range(int(max(list(xytrain[:, -3]))) + 1, int(max(list(xytrain[:, -2]))) + 1)\n",
        "        range_words = range(int(max(list(xytrain[:, -2]))) + 1, vocabulary_size)\n",
        "        self.min_ranges = [min(range_times), min(range_coors), min(range_words)]\n",
        "        \n",
        "        tuples_train = tuples_train[:5000]\n",
        "        numexamples = len(tuples_train)\n",
        "        print(numexamples)\n",
        "        \n",
        "        seq_train = self.transform_as_seq(tuples_train)\n",
        "        \n",
        "        losses = []\n",
        "        loss_function_text = nn.NLLLoss()\n",
        "        loss_function = torch.nn.CrossEntropyLoss()\n",
        "      \n",
        "        embed = Embed(vocabulary_size, embedding_dims).to(device)\n",
        "        optimizer_emb = optim.Adam(embed.parameters(), lr=0.001)\n",
        "        \n",
        "        encoder = EncoderRNN(embedding_dims, hidden_size, embed).to(device)\n",
        "        optimizer_enc = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "        \n",
        "        decoder = DecoderRNN(embedding_dims, hidden_size, embed, range_times, range_coors, range_words).to(device)\n",
        "        optimizer_dec = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "        \n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0\n",
        "            \n",
        "            #for i in seq_train:\n",
        "            for i in [0,1,2]:\n",
        "              \n",
        "              for (input_seq,target) in seq_train[i]:\n",
        "                optimizer_dec.zero_grad()\n",
        "                optimizer_enc.zero_grad() \n",
        "                #optimizer_emb.zero_grad()\n",
        "                \n",
        "                #print(input_seq)\n",
        "                #print(target)\n",
        "                \n",
        "                loss = 0\n",
        "                input_length = len(input_seq)\n",
        "                encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
        "                input_seq = torch.tensor(input_seq, device=device)\n",
        "                \n",
        "                encoder_hidden = encoder.initHidden()\n",
        "                for ei in range(input_length):\n",
        "                  encoder_output, encoder_hidden = encoder(input_seq[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] = encoder_output[0, 0]\n",
        "                \n",
        "                decoder_hidden = encoder_hidden\n",
        "                decoder_input = encoder_outputs[input_length-1].view(1, 1, -1)\n",
        "                \n",
        "                if (i == 2):\n",
        "                  target_length = len(target)\n",
        "                  decoder_outputs = torch.zeros(target_length, decoder.hidden_size, device=device)\n",
        "                  target_seq = torch.tensor(target, device=device)\n",
        "                  \n",
        "                  \n",
        "                  for di in range(target_length):\n",
        "                    input = torch.tensor([target_seq[di]], device=device)\n",
        "                    decoder_output, decoder_hidden = decoder(input, decoder_hidden, i)\n",
        "                    loss += loss_function_text(decoder_output, input)\n",
        "                    #decoder_input = target[di] \n",
        "                    decoder_outputs[di] = decoder_output[0, 0]\n",
        "                else:\n",
        "                  y = torch.tensor(target, dtype=torch.long)\n",
        "                  log_probs = decoder(decoder_input, decoder_hidden, i)\n",
        "                  loss = loss_function(log_probs, y.to(device))\n",
        "                \n",
        "                loss.backward()\n",
        "                total_loss += loss.item()\n",
        "                optimizer_dec.step()\n",
        "                optimizer_enc.step() \n",
        "                #optimizer_emb.step()\n",
        "                \n",
        "                encoder_hidden.detach_()\n",
        "            #if epoch % (num_epochs//10 +1) == 0:\n",
        "            #  print(total_loss/numexamples)\n",
        "            print(total_loss/numexamples)\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "    def fit_embed(self, tuples_train, embedding_dims, num_epochs = 1, hidden_size=100):\n",
        "        xytrain = self.transform_as_nparray(tuples_train)\n",
        "        vocabulary_size = int(max(list(xytrain[:, -1]))) + 1\n",
        "        \n",
        "        self.padding = vocabulary_size\n",
        "        vocabulary_size += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "        range_times = range(int(max(list(xytrain[:, -3]))) + 1)\n",
        "        range_coors = range(int(max(list(xytrain[:, -3]))) + 1, int(max(list(xytrain[:, -2]))) + 1)\n",
        "        range_words = range(int(max(list(xytrain[:, -2]))) + 1, vocabulary_size)\n",
        "        self.min_ranges = [min(range_times), min(range_coors), min(range_words)]\n",
        "        \n",
        "        tuples_train = tuples_train[:5000]\n",
        "        numexamples = len(tuples_train)\n",
        "        print(numexamples)\n",
        "        \n",
        "        seq_train = self.transform_as_seq(tuples_train)\n",
        "        \n",
        "        losses = []\n",
        "        loss_function_text = nn.NLLLoss()\n",
        "        loss_function = torch.nn.CrossEntropyLoss()\n",
        "      \n",
        "        embed = Embed(vocabulary_size, embedding_dims).to(device)\n",
        "        optimizer_emb = optim.Adam(embed.parameters(), lr=0.001)\n",
        "        \n",
        "        encoder = EncoderRNN(embedding_dims, hidden_size, embed).to(device)\n",
        "        optimizer_enc = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "        \n",
        "        decoder = DecoderRNN(embedding_dims, hidden_size, embed, range_times, range_coors, range_words).to(device)\n",
        "        optimizer_dec = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "        \n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0\n",
        "            \n",
        "            #for i in seq_train:\n",
        "            for i in [0,1,2]:\n",
        "              \n",
        "              for (input_seq,target) in seq_train[i]:\n",
        "                optimizer_dec.zero_grad()\n",
        "                optimizer_enc.zero_grad() \n",
        "                #optimizer_emb.zero_grad()\n",
        "                \n",
        "                #print(input_seq)\n",
        "                #print(target)\n",
        "                \n",
        "                loss = 0\n",
        "                input_length = len(input_seq)\n",
        "                encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
        "                input_seq = torch.tensor(input_seq, device=device)\n",
        "                \n",
        "                encoder_hidden = encoder.initHidden()\n",
        "                for ei in range(input_length):\n",
        "                  encoder_output, encoder_hidden = encoder(input_seq[ei], encoder_hidden)\n",
        "                  encoder_outputs[ei] = encoder_output[0, 0]\n",
        "                \n",
        "                decoder_hidden = encoder_hidden\n",
        "                decoder_input = encoder_outputs[input_length-1].view(1, 1, -1)\n",
        "                \n",
        "                if (i == 2):\n",
        "                  target_length = len(target)\n",
        "                  decoder_outputs = torch.zeros(target_length, decoder.hidden_size, device=device)\n",
        "                  target_seq = torch.tensor(target, device=device)\n",
        "                  \n",
        "                  \n",
        "                  for di in range(target_length):\n",
        "                    input = torch.tensor([target_seq[di]], device=device)\n",
        "                    decoder_output, decoder_hidden = decoder(input, decoder_hidden, i)\n",
        "                    loss += loss_function_text(decoder_output, input)\n",
        "                    #decoder_input = target[di] \n",
        "                    decoder_outputs[di] = decoder_output[0, 0]\n",
        "                else:\n",
        "                  y = torch.tensor(target, dtype=torch.long)\n",
        "                  log_probs = decoder(decoder_input, decoder_hidden, i)\n",
        "                  loss = loss_function(log_probs, y.to(device))\n",
        "                \n",
        "                loss.backward()\n",
        "                total_loss += loss.item()\n",
        "                optimizer_dec.step()\n",
        "                optimizer_enc.step() \n",
        "                #optimizer_emb.step()\n",
        "                \n",
        "                encoder_hidden.detach_()\n",
        "            #if epoch % (num_epochs//10 +1) == 0:\n",
        "            #  print(total_loss/numexamples)\n",
        "            print(total_loss/numexamples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qz4hlyr2tFOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "69aad882-8edd-4edf-927a-6969f1f48fc6"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "translate = {'w':'Text','l':'Location','t':'Time'}\n",
        "trainsets = [trainST, trainLA, trainNY]\n",
        "testsets = [testST, testLA, testNY]\n",
        "namesets = ['Santiago','LA','NY']\n",
        "results = {'Model':[],'Dataset':[],  'Text':[], 'Location':[],'Time':[]}\n",
        "\n",
        "\n",
        "for train,test,name in list(zip(trainsets, testsets, namesets))[:1]:\n",
        "    \n",
        "    dims = [100]\n",
        "    \n",
        "    model = SimpleEmbedding()\n",
        "    \n",
        "    \n",
        "    for dim in dims:\n",
        "        model.fit(train, embedding_dims=dim, num_epochs=10, hidden_size=dim)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e3140f1793d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"translate = {'w':'Text','l':'Location','t':'Time'}\\ntrainsets = [trainST, trainLA, trainNY]\\ntestsets = [testST, testLA, testNY]\\nnamesets = ['Santiago','LA','NY']\\nresults = {'Model':[],'Dataset':[],  'Text':[], 'Location':[],'Time':[]}\\n\\n\\nfor train,test,name in list(zip(trainsets, testsets, namesets))[:1]:\\n    QE = QuantitativeEvaluator()\\n    \\n    dims = [100]\\n    \\n    model = SimpleEmbedding()\\n    \\n    \\n    for dim in dims:\\n        model.fit(train, embedding_dims=dim, num_epochs=10, hidden_size=dim)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'QuantitativeEvaluator' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zAMY5mqPkKvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}