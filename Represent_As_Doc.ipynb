{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Represent_As_Doc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juglar-diaz/STTD/blob/master/Represent_As_Doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c02j_-p6TZ2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Intro"
      ]
    },
    {
      "metadata": {
        "id": "R3QjBnH6lktK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJWrJduhlnI-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGPfqOgRnsBQ",
        "colab_type": "code",
        "outputId": "39926f4b-c447-4173-92e2-3c8160b3730f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "!pip3 install gensim\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.28)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.28 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.28->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.28->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lboU3CzKkHsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6127f56b-f7ce-44cf-9011-f0bf55bf2f0a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import itertools\n",
        "from gensim import models\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "sep = os.sep\n",
        "import os.path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "if(os.path.isfile('utils.py')):\n",
        "    !rm 'utils.py'\n",
        "    print(\"deleted\")\n",
        "\n",
        "exchangedrive = drive.CreateFile({'id':'12djgLg96tD7ynEhImcqoYnL5ZamenDEM'})\n",
        "exchangedrive.GetContentFile('utils.py')\n",
        "\n",
        "from utils import *\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deleted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s6m5UOXClc5v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Download Data"
      ]
    },
    {
      "metadata": {
        "id": "Nrks3-MplPnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exchangedrive = drive.CreateFile({'id':'1cMCzlvTMlUPgaYaUIdlMVtTpp8r10GnX'})\n",
        "exchangedrive.GetContentFile('robosclean.p')\n",
        "\n",
        "\n",
        "exchangedrive = drive.CreateFile({'id':'1A5Wa6LiaGs8XeW2S2qjGbcoSSlsMGW97'})\n",
        "exchangedrive.GetContentFile('tweetsLAtrain.csv')\n",
        "exchangedrive = drive.CreateFile({'id':'1CrUCS7oWzdvYoFwtWgikiGkDAu6w3dOF'})\n",
        "exchangedrive.GetContentFile('tweetsLAtest.csv')\n",
        "exchangedrive = drive.CreateFile({'id':'1AU_saqdyRxtgMX2TC7Q42OBIUm68cdw_'})\n",
        "exchangedrive.GetContentFile('tweetsLA.csv')\n",
        "\n",
        "\n",
        "\n",
        "exchangedrive = drive.CreateFile({'id':'1NuSVM7-h0CCRtzi0woM4h5tVd6T7JO2C'})\n",
        "exchangedrive.GetContentFile('tweetsNYtrain.csv')\n",
        "exchangedrive = drive.CreateFile({'id':'1UYZY0sh1-Q8MIofMAHDGaNKv8cfDA0ui'})\n",
        "exchangedrive.GetContentFile('tweetsNYtest.csv')\n",
        "\n",
        "\n",
        "exchangedrive = drive.CreateFile({'id':'1Jv19eJTZwsZWEA_rUudvKwcn1TS-DFBa'})\n",
        "exchangedrive.GetContentFile('tweets2016_2half.csv')\n",
        "exchangedrive = drive.CreateFile({'id':'1E8WlhOXb3tfQbLUpizx7QXzeydv57eXN'})\n",
        "exchangedrive.GetContentFile('toy_2017_Jan.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKDUM7jvCQJb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Models"
      ]
    },
    {
      "metadata": {
        "id": "7S5_mCHKVqul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RepreAsDoc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def fit(self, tuples_train, tf_idf = False, lda = False):#t is a tuple (date_out, coor_out, tweets_out)\n",
        "\n",
        "        ldates_as_doc = []# a list of [wordidx,...,wordidx]), where the index of the list is a dateidx or a cooridx or a wordidx\n",
        "        lcoor_as_doc = []# a list of [wordidx,...,wordidx]), where the index of the list is a cooridx\n",
        "\n",
        "        self.max_idxdate = max([tuple[0] for tuple in tuples_train])\n",
        "        #print(self.max_idxdate)\n",
        "        self.min_idxword = max([tuple[1] for tuple in tuples_train]) + 1\n",
        "        #print(self.min_idxword)\n",
        "\n",
        "\n",
        "        max_idxword = 0\n",
        "        for tuple in tuples_train:\n",
        "            for idxword in tuple[2]:\n",
        "                if idxword > max_idxword:\n",
        "                    max_idxword = idxword\n",
        "        #print(max_idxword)\n",
        "        self.max_idxword = max_idxword\n",
        "\n",
        "\n",
        "        min_idxword = max_idxword\n",
        "        for tuple in tuples_train:\n",
        "            for idxword in tuple[2]:\n",
        "                if idxword < min_idxword:\n",
        "                    min_idxword = idxword\n",
        "        #print(min_idxword)\n",
        "\n",
        "        vectors = [[] for _ in range(max_idxword+1)]\n",
        "\n",
        "        for i in range(max_idxword+1):\n",
        "            vectors[i] = [0 for _ in range(max_idxword-min_idxword + 1)]\n",
        "\n",
        "        for tuple in tuples_train:\n",
        "            dateidx, cooridx, wordsidxs = tuple[0], tuple[1], tuple[2]\n",
        "            for idxword in wordsidxs:\n",
        "                vectors[dateidx][idxword-min_idxword] += 1\n",
        "                vectors[cooridx][idxword-min_idxword] += 1\n",
        "\n",
        "                for word_idx in wordsidxs:\n",
        "                    vectors[word_idx][idxword-min_idxword] += 1\n",
        "\n",
        "        if(tf_idf and not lda):\n",
        "            tfidf_model = TfidfTransformer()\n",
        "            vectors = tfidf_model.fit_transform(vectors)\n",
        "            vectors = vectors.todense()\n",
        "\n",
        "        if(tf_idf and lda):\n",
        "            tfidf_model = TfidfTransformer()\n",
        "            vectors = tfidf_model.fit_transform(vectors)\n",
        "\n",
        "            lda_model = LatentDirichletAllocation(n_components=20, max_iter=5,\n",
        "                                            learning_method='online',\n",
        "                                            learning_offset=50.,\n",
        "                                            random_state=0)\n",
        "            vectors = lda_model.fit_transform(vectors)\n",
        "\n",
        "        if (not tf_idf and lda):\n",
        "\n",
        "            vectors = csr_matrix(vectors)\n",
        "\n",
        "            lda_model = LatentDirichletAllocation(n_components=20, max_iter=5,\n",
        "                                                  learning_method='online',\n",
        "                                                  learning_offset=50.,\n",
        "                                                  random_state=0)\n",
        "            vectors = lda_model.fit_transform(vectors)\n",
        "\n",
        "        self.W = np.array(vectors)\n",
        "\n",
        "    def predict(self, t_idx, l_idx, w_idx):\n",
        "        vec_t = self.W[t_idx].reshape(1, -1)\n",
        "        vec_l = self.W[l_idx].reshape(1, -1)\n",
        "\n",
        "        vec_w = np.average([self.W[idx].reshape(1, -1) for idx in w_idx], axis=0)\n",
        "\n",
        "        vectors = [vec_t, vec_l, vec_w]\n",
        "        #print(vec_t.shape, vec_l.shape, vec_w.shape)\n",
        "        #for vec1, vec2 in itertools.combinations(vectors, r=2):\n",
        "        #    if (cosine_similarity(vec1, vec2).shape != (1, 1)):\n",
        "        #        print('cosine_similarity shape error')\n",
        "        #    print(cosine_similarity(vec1, vec2))\n",
        "        score = sum([cosine_similarity(vec1, vec2)[0][0] for vec1, vec2 in itertools.combinations(vectors, r=2)])\n",
        "        return round(score, 6)\n",
        "\n",
        "    def getPlaces(self):\n",
        "        return self.W[self.max_idxdate+1:self.min_idxword,:]\n",
        "\n",
        "    def queryItem(self, idxs, number= 10):\n",
        "        query_vectors = self.W[idxs]\n",
        "        query_vectors.reshape(query_vectors.shape[0],1,-1)\n",
        "        #print(self.max_idxdate+1)\n",
        "\n",
        "        date_vectors = self.W[list(range(0,self.max_idxdate+1))]\n",
        "        #print(date_vectors)\n",
        "\n",
        "        date_vectors.reshape(date_vectors.shape[0], 1, -1)\n",
        "\n",
        "        cosines_sim_date = cosine_similarity(query_vectors, date_vectors)\n",
        "\n",
        "\n",
        "\n",
        "        #coor_vectors = self.W[list(range(self.max_idxdate+1,self.min_idxword))]\n",
        "        coor_vectors = self.W[self.max_idxdate+1:self.min_idxword,:]\n",
        "        #print(coor_vectors)\n",
        "\n",
        "        coor_vectors.reshape(coor_vectors.shape[0], 1, -1)\n",
        "        cosines_sim_coor = cosine_similarity(query_vectors, coor_vectors)\n",
        "\n",
        "        word_vectors = self.W[list(range(self.min_idxword, self.max_idxword + 1))]\n",
        "        word_vectors.reshape(word_vectors.shape[0], 1, -1)\n",
        "        cosines_sim_word = cosine_similarity(query_vectors, word_vectors)\n",
        "\n",
        "\n",
        "        indexes_date = np.argsort(cosines_sim_date, )\n",
        "        indexes_coor = np.argsort(cosines_sim_coor) + self.max_idxdate+1\n",
        "        indexes_word = np.argsort(cosines_sim_word) + self.min_idxword\n",
        "\n",
        "        return (indexes_date[:1,-number:], indexes_coor[:1,-number:], indexes_word[:1,-number:])\n",
        "\n",
        "    def queryVector(self, query_vectors, number= 10):\n",
        "        query_vectors = np.array(query_vectors)\n",
        "        query_vectors.reshape(query_vectors.shape[0],1,-1)\n",
        "        #print(self.max_idxdate+1)\n",
        "\n",
        "        date_vectors = self.W[list(range(0,self.max_idxdate+1))]\n",
        "        #print(date_vectors)\n",
        "\n",
        "        date_vectors.reshape(date_vectors.shape[0], 1, -1)\n",
        "\n",
        "        cosines_sim_date = cosine_similarity(query_vectors, date_vectors)\n",
        "\n",
        "\n",
        "\n",
        "        #coor_vectors = self.W[list(range(self.max_idxdate+1,self.min_idxword))]\n",
        "        coor_vectors = self.W[self.max_idxdate+1:self.min_idxword,:]\n",
        "        #print(coor_vectors)\n",
        "\n",
        "        coor_vectors.reshape(coor_vectors.shape[0], 1, -1)\n",
        "        cosines_sim_coor = cosine_similarity(query_vectors, coor_vectors)\n",
        "\n",
        "        word_vectors = self.W[list(range(self.min_idxword, self.max_idxword + 1))]\n",
        "        word_vectors.reshape(word_vectors.shape[0], 1, -1)\n",
        "        cosines_sim_word = cosine_similarity(query_vectors, word_vectors)\n",
        "\n",
        "\n",
        "        indexes_date = np.argsort(cosines_sim_date, )\n",
        "        indexes_coor = np.argsort(cosines_sim_coor) + self.max_idxdate+1\n",
        "        indexes_word = np.argsort(cosines_sim_word) + self.min_idxword\n",
        "\n",
        "        return (indexes_date[:1,-number:], indexes_coor[:1,-number:], indexes_word[:1,-number:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F209VamXR-ov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Eval"
      ]
    },
    {
      "metadata": {
        "id": "VSZCBPT-rMme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "e691b8eb-7271-40a7-9a8a-336cc530ab8c"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "translate = {'w':'Text','l':'Location','t':'Time'}\n",
        "\n",
        "namesets = ['Santiago','LA']\n",
        "results = {'Model':[],'Dataset':[],  'Text':[], 'Location':[],'Time':[]}\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "    trainST,word2idxST,testST = chargeData_SplitTrainTest('tweets2016_2half.csv')\n",
        "    trainLA,word2idxLA,testLA = chargeData_SplitTrainTest(\"tweetsLA.csv\")\n",
        "    \n",
        "    trainsets = [trainST,  trainLA]\n",
        "    testsets = [testST,  testLA]\n",
        "    for train,test,name in list(zip(trainsets, testsets, namesets)):\n",
        "        QE = QuantitativeEvaluator()\n",
        "\n",
        "\n",
        "        model = RepreAsDoc()\n",
        "\n",
        "        ldadict={True:'lda', False:''}\n",
        "        tfidfdict={True:'tf_idf', False:''}\n",
        "\n",
        "        for lda in [False, True]:\n",
        "            for tfidf in [False, True]:\n",
        "                print(lda,tfidf)\n",
        "                model.fit(train, lda=lda, tf_idf=tfidf)\n",
        "\n",
        "                print('Done Training')\n",
        "\n",
        "\n",
        "                for predict_type in 'wlt':\n",
        "                    QE.get_ranks(test, model, predict_type = predict_type)\n",
        "                    mrr1 = QE.compute_mrr()[0]\n",
        "                    print(f\"{name} MRR {predict_type} {mrr1}\")\n",
        "                    results[translate[predict_type]].append(mrr1)\n",
        "                results['Dataset'].append(name)\n",
        "                results['Model'].append(str(type(model))+'_'+tfidfdict[tfidf]+'_'+ldadict[lda])\n",
        "                df = pd.DataFrame(results)\n",
        "\n",
        "                df.to_csv('resultsRobos.df')\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n",
            "Done Training\n",
            "Santiago MRR w 0.6049\n",
            "Santiago MRR l 0.446\n",
            "Santiago MRR t 0.3959\n",
            "False True\n",
            "Done Training\n",
            "Santiago MRR w 0.449\n",
            "Santiago MRR l 0.4356\n",
            "Santiago MRR t 0.4147\n",
            "True False\n",
            "Done Training\n",
            "Santiago MRR w 0.4261\n",
            "Santiago MRR l 0.4276\n",
            "Santiago MRR t 0.4128\n",
            "True True\n",
            "Done Training\n",
            "Santiago MRR w 0.4042\n",
            "Santiago MRR l 0.3933\n",
            "Santiago MRR t 0.3828\n",
            "False False\n",
            "Done Training\n",
            "LA MRR w 0.4468\n",
            "LA MRR l 0.4316\n",
            "LA MRR t 0.3904\n",
            "False True\n",
            "Done Training\n",
            "LA MRR w 0.4123\n",
            "LA MRR l 0.42\n",
            "LA MRR t 0.4021\n",
            "True False\n",
            "Done Training\n",
            "LA MRR w 0.4076\n",
            "LA MRR l 0.4077\n",
            "LA MRR t 0.3967\n",
            "True True\n",
            "Done Training\n",
            "LA MRR w 0.3847\n",
            "LA MRR l 0.3752\n",
            "LA MRR t 0.3662\n",
            "CPU times: user 5h 42min 53s, sys: 4min 34s, total: 5h 47min 27s\n",
            "Wall time: 5h 41min 57s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_jCz-uRKSYyq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "endrun"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KVBEdZ9rVAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s8F-UTzH-mHF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "translate = {'w':'Text','l':'Location','t':'Time'}\n",
        "\n",
        "namesets = ['Santiago','LA']\n",
        "results = {'Model':[],'Dataset':[],  'Text':[], 'Location':[],'Time':[],'Batch':[],'Dim':[]}\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "    trainST,word2idxST,testST = chargeData_SplitTrainTest('tweets2016_2half.csv')\n",
        "    trainLA,word2idxLA,testLA = chargeData_SplitTrainTest(\"tweetsLA.csv\")\n",
        "    \n",
        "    trainsets = [trainST,  trainLA]\n",
        "    testsets = [testST,  testLA]\n",
        "    for train,test,name in list(zip(trainsets, testsets, namesets)):\n",
        "\n",
        "        dims = [20,50,100,200,400]\n",
        "        batchs = [200]\n",
        "\n",
        "\n",
        "        for dim in dims:\n",
        "            for batch in batchs:\n",
        "                model = SimpleRNN()\n",
        "                model.fit_batch(train, embedding_dims=dim, num_epochs=5, hidden_size=dim, batch_size=batch)\n",
        "                print(f\"Test size {len(test)}\")\n",
        "                for predict_type in 'wlt':\n",
        "                    resultsp, results_targets = model.predict(test, batch_test=10000, predict_type=predict_type, fake = 10)\n",
        "                    mrr1 = model.mrr(resultsp, results_targets, predict_type=predict_type)[0]\n",
        "                    print(f\"{name} MRR {predict_type} {mrr1} {dim} {batch}\")\n",
        "                    results[translate[predict_type]].append(mrr1)\n",
        "\n",
        "                results['Dataset'].append(name)\n",
        "                results['Dim'].append(dim)\n",
        "                results['Batch'].append(batch)\n",
        "\n",
        "                results['Model'].append(str(type(model))+'_'+str())\n",
        "\n",
        "                df = pd.DataFrame(results)\n",
        "\n",
        "                df.to_csv('results.df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efCy5Mvt-mZg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8UtdbGcw91r8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "translate = {'w':'Text','l':'Location','t':'Time'}\n",
        "\n",
        "namesets = ['Santiago','LA']\n",
        "results = {'Model':[],'Dataset':[],  'Text':[], 'Location':[],'Time':[],'Batch':[],'Dim':[]}\n",
        "\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    trainST,word2idxST,testST = chargeData_SplitTrainTest('tweets2016_2half.csv')\n",
        "    trainLA,word2idxLA,testLA = chargeData_SplitTrainTest(\"tweetsLA.csv\")\n",
        "    \n",
        "    trainsets = [trainST,  trainLA]\n",
        "    testsets = [testST,  testLA]\n",
        "    for train,test,name in list(zip(trainsets, testsets, namesets)):\n",
        "\n",
        "        dims = [100]\n",
        "        batchs = [200]\n",
        "\n",
        "\n",
        "        for dim in dims:\n",
        "            for batch in batchs:\n",
        "                model = SimpleRNN()\n",
        "                model.fit_batch(train, embedding_dims=dim, num_epochs=5, hidden_size=dim, batch_size=batch)\n",
        "                print(f\"Test size {len(test)}\")\n",
        "                for predict_type in 'wlt':\n",
        "                    resultsp, results_targets = model.predict(test, batch_test=10000, predict_type=predict_type, fake = 10)\n",
        "                    mrr1 = model.mrr(resultsp, results_targets, predict_type=predict_type)[0]\n",
        "                    print(f\"{name} MRR {predict_type} {mrr1} {dim} {batch}\")\n",
        "                    results[translate[predict_type]].append(mrr1)\n",
        "\n",
        "                results['Dataset'].append(name)\n",
        "                results['Dim'].append(dim)\n",
        "                results['Batch'].append(batch)\n",
        "\n",
        "                results['Model'].append(str(type(model))+'_'+str())\n",
        "\n",
        "                df = pd.DataFrame(results)\n",
        "\n",
        "                df.to_csv('results.df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sSme1S1U913f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDBRqZQXYT6u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "translate = {'w':'Text','l':'Location','t':'Time'}\n",
        "trainsets = [trainST, trainNY, trainLA]\n",
        "testsets = [testST, testNY, testLA]\n",
        "namesets = ['Santiago','NY','LA']\n",
        "results = {'Model':[],'Dataset':[],  'Text':[], 'Location':[],'Time':[],'Batch':[],'Dim':[]}\n",
        "\n",
        "for train,test,name in list(zip(trainsets, testsets, namesets)):\n",
        "    \n",
        "    dims = [100]\n",
        "    batchs = [200]\n",
        "    \n",
        "    \n",
        "    for dim in dims:\n",
        "        for batch in batchs:\n",
        "            model = SimpleRNN()\n",
        "            model.fit_batch(train, embedding_dims=dim, num_epochs=10, hidden_size=dim, batch_size=batch)\n",
        "            print(f\"Test size {len(test)}\")\n",
        "            for predict_type in 'ltw':\n",
        "                resultsp, results_targets = model.predict(test, batch_test=10000, predict_type=predict_type, fake = 10)\n",
        "                mrr1 = model.mrr(resultsp, results_targets, predict_type=predict_type)[0]\n",
        "                print(f\"{name} MRR {predict_type} {mrr1} {dim} {batch}\")\n",
        "                results[translate[predict_type]].append(mrr1)\n",
        "\n",
        "            results['Dataset'].append(name)\n",
        "            results['Dim'].append(dim)\n",
        "            results['Batch'].append(batch)\n",
        "\n",
        "            results['Model'].append(str(type(model))+'_'+str())\n",
        "\n",
        "            df = pd.DataFrame(results)\n",
        "\n",
        "            df.to_csv('results.df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEd1wP2goxEi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}